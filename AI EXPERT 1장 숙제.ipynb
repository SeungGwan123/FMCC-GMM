{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI EXPERT 1장 과제\n",
    "\n",
    "이승관 브랜치 연습\n",
    "\n",
    "원본출처: https://github.com/wikibook/rlnn\n",
    "\n",
    "## 아래 코드를 실행해 보면서, 실행 결과와 각각에 대해서 적혀있는 질문의 대답에 대해서 PPT를 만들어 보세요.\n",
    ".\n",
    "## 질문은 언제나 카톡 으로 환영합니다.\n",
    "\n",
    "\n",
    "### 다음과 같은 패키지를 설치해야 합니다\n",
    "- numpy\n",
    "- matplotlib\n",
    "\n",
    "### conda를 사용한다면 설치방법\n",
    "- conda install numpy matplotlib\n",
    "\n",
    "### pip 을 사용한다면 설치방법\n",
    "- pip install numpy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001B[1;31m자세한 내용을 보려면 <a href='https://aka.ms/kernelFailuresDllLoad'>여기</a>를 클릭합니다."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그림그리는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_v_table_small(v_table, env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1])\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1])\n",
    "\n",
    "# V table 그리기    \n",
    "def show_v_table(v_table, env):    \n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# Q table 그리기\n",
    "def show_q_table(q_table,env):\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n",
    "                if k==1:\n",
    "                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "\n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_q_table_arrow(q_table,env):\n",
    "    q = q_table\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,0]:\n",
    "                        print(\"        ↑       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==1:                    \n",
    "                    if np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←  →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,1]:\n",
    "                        print(\"          →     |\",end=\"\")\n",
    "                    elif np.max(q[i,j,:]) == q[i,j,3]:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "                if k==2:\n",
    "                    if np.max(q[i,j,:]) == q[i,j,2]:\n",
    "                        print(\"        ↓       |\",end=\"\")\n",
    "                    else:\n",
    "                        print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")    \n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy_small(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        print(\"|\", end=\"\")\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"road\":\n",
    "                if policy[i,j] == 0:\n",
    "                    print(\"   ↑     |\",end=\"\")\n",
    "                elif policy[i,j] == 1:\n",
    "                    print(\"   →     |\",end=\"\")\n",
    "                elif policy[i,j] == 2:\n",
    "                    print(\"   ↓     |\",end=\"\")\n",
    "                elif policy[i,j] == 3:\n",
    "                    print(\"   ←     |\",end=\"\")\n",
    "            else:\n",
    "                print(\"          |\",end=\"\")\n",
    "        print()\n",
    "    print(\"+----------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")\n",
    "    \n",
    "# 정책 policy 화살표로 그리기\n",
    "def show_policy(policy,env):\n",
    "    for i in range(env.reward.shape[0]):        \n",
    "        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "        print(\"+\")\n",
    "        for k in range(3):\n",
    "            print(\"|\",end=\"\")\n",
    "            for j in range(env.reward.shape[1]):\n",
    "                if k==0:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "                if k==1:\n",
    "                    if policy[i,j] == 0:\n",
    "                        print(\"      ↑         |\",end=\"\")\n",
    "                    elif policy[i,j] == 1:\n",
    "                        print(\"      →         |\",end=\"\")\n",
    "                    elif policy[i,j] == 2:\n",
    "                        print(\"      ↓         |\",end=\"\")\n",
    "                    elif policy[i,j] == 3:\n",
    "                        print(\"      ←         |\",end=\"\")\n",
    "                if k==2:\n",
    "                    print(\"                 |\",end=\"\")\n",
    "            print()\n",
    "    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n",
    "    print(\"+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n",
    "    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n",
    "    \n",
    "    # 2. 각 행동별 선택확률\n",
    "    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n",
    "    \n",
    "    # 3. 에이전트의 초기 위치 저장\n",
    "    def __init__(self):\n",
    "        self.pos = (0,0)\n",
    "    \n",
    "    # 4. 에이전트의 위치 저장\n",
    "    def set_pos(self,position):\n",
    "        self.pos = position\n",
    "        return self.pos\n",
    "    \n",
    "    # 5. 에이전트의 위치 불러오기\n",
    "    def get_pos(self):\n",
    "        return self.pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n",
    "    cliff = -3\n",
    "    road = -1\n",
    "    goal = 1\n",
    "    \n",
    "    # 2. 목적지 좌표 설정\n",
    "    goal_position = [1,1]\n",
    "    \n",
    "    # 3. 보상 리스트 숫자\n",
    "    reward_list = [[road,road],\n",
    "                   [road,goal]]\n",
    "    \n",
    "    # 4. 보상 리스트 문자\n",
    "    reward_list1 = [[\"road\",\"road\"],\n",
    "                    [\"road\",\"goal\"]]\n",
    "    \n",
    "    # 5. 보상 리스트를 array로 설정\n",
    "    def __init__(self):\n",
    "        self.reward = np.asarray(self.reward_list)    \n",
    "\n",
    "    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n",
    "    def move(self, agent, action):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        # 6.1 행동에 따른 좌표 구하기\n",
    "        new_pos = agent.pos + agent.action[action]\n",
    "        \n",
    "        # 6.2 현재좌표가 목적지 인지확인\n",
    "        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "            reward = self.goal\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.3 이동 후 좌표가 미로 밖인 확인    \n",
    "        elif self.reward_list1[agent.pos[0]][agent.pos[1]] == \"cliff\" or\\\n",
    "            new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n",
    "            reward = self.cliff\n",
    "            observation = agent.set_pos(agent.pos)\n",
    "            done = True\n",
    "        # 6.4 이동 후 좌표가 길이라면\n",
    "        else:\n",
    "            observation = agent.set_pos(new_pos)\n",
    "            reward = self.reward[observation[0],observation[1]]\n",
    "            \n",
    "        return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 상태 가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 가치 계산\n",
    "def state_value_function(env,agent,G,max_step,now_step):\n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "# 2. 현재 위치가 도착지점인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "    \n",
    "# 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 3.1 가능한 모든 행동의 보상을 계산\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            observation, reward, done = env.move(agent,i)\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "            \n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n",
    "    else:\n",
    "        \n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            observation, reward, done = env.move(agent,i)      \n",
    "            # 4.2.1 현재 상태에서 보상을 계산\n",
    "            G += agent.select_action_pr[i] * reward\n",
    "\n",
    "            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "            if done == True:\n",
    "                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                    agent.set_pos(pos1)\n",
    "\n",
    "            # 4.2.3 다음 step을 계산\n",
    "            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "\n",
    "            # 4.2.4 현재 위치를 복구\n",
    "            agent.set_pos(pos1)\n",
    "\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미로의 각 상태의 상태가치함수를 구하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -2.00      |      -1.50      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -1.50      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.58      |      -2.40      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -2.40      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.69      |      -3.16      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.16      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 3 total_time = 0.0(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.53      |      -3.75      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -3.75      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 4 total_time = 0.01(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -6.18      |      -4.21      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.21      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 5 total_time = 0.03(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -6.67      |      -4.56      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.56      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 6 total_time = 0.11(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.05      |      -4.83      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -4.83      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 7 total_time = 0.39(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.35      |      -5.03      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.03      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "max_step_number = 8 total_time = 1.32(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.57      |      -5.19      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.19      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 9 total_time = 4.38(s)\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -7.74      |      -5.32      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|      -5.32      |       1.00      |\n",
      "|                 |                 |\n",
      "+-----------------+-----------------+\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "\n",
    "# 3. 최대 max_step_number 제한\n",
    "max_step_number = 10\n",
    "\n",
    "# 4. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "# 5. 재귀함수 state_value_function을를 이용해 각 상태 가치를 계산\n",
    "for max_step in range(max_step_number):\n",
    "    \n",
    "    # 5.1 미로 각 상태의 가치를 테이블 형식으로 저장\n",
    "    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 5.2 미로의 각 상태에 대해 state_value_function() 을 이용해 가치를 계산한 후 테이블 형식으로 저장\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            agent.set_pos([i,j])\n",
    "            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n",
    "            \n",
    "    # 5.3 max_down에 따른 계산시간 저장\n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "    \n",
    "    show_v_table(np.round(v_table,2),env)\n",
    "\n",
    "# 6. step 별 계산 시간 그래프 그리기    \n",
    "plt.plot(time_len, 'o-k')\n",
    "plt.xlabel('max_down')\n",
    "plt.ylabel('time(s)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 코드까지는 각 상태에서 최대 스텝 제한을 두어 벨만 방정식을 따른 상태 가치 함수의 근사값을 구하는 과정이다.\n",
    "\n",
    "상태는 (0, 0), (0, 1), (1, 0), (1, 1)이 있다.\n",
    "\n",
    "행동은 상, 하, 좌, 우 이동으로 각 확률은 25%로 균등하다.\n",
    "\n",
    "보상은 road로 가는 행동은 -1, goal로 가는 행동은 1이며, 맵 바깥을 벗어나거나 절벽으로 가는 행동은 -3 이다.\n",
    "\n",
    "맵 바깥을 벗어나거나 절벽으로 가면 행동하기 전 위치로 상태가 된다.\n",
    "\n",
    "\n",
    "특정 상태에서의 벨만 방정식 상태 가치 함수를 구할 때는, (엄밀하게 정의하자면) 무한한 행동을 취했을 때의 보상의 총합의 기댓값이다. 실제로는 무한한 행동을 구현할 수는 없으므로, 이 코드에서는 max_step_number로 행동 횟수 제한을 두었다 (이 코드에서 max_step_number=0 이면 행동 1번만 고려한다는 의미이다).\n",
    "\n",
    "max_step_number가 크면 클 수록 엄밀한 벨만방정식 상태 가치 함수에 가까워질 것이다.\n",
    "\n",
    "### 질문 1-1. 위 코드 내용을 기반으로 상태는 (0, 0), (0, 1), (1, 0), (1, 1)에 대한 마르코프 체인 네트워크와 상태 전이 매트릭스를 그려보자. 옳게 그린다면, 상태 전이 매트릭스는 4x4 행렬이어야 한다\n",
    "\n",
    "### 질문 1-2. (1.)에서 그린 상태 전이 매트릭스를 기반으로, max_step_number = 0일 때 (즉, 행동 한 번만 고려할 때) 상태 (0, 0), (0, 1), (1, 0)에서에 벨만 방정식 상태 가치 함수의 근사값을 수식으로 구하는 과정을 보여보자.\n",
    "\n",
    "예: V(X=(0, 0)) = P_(0,0)(0,0) * R_(0,0) + P_(0,0)(0,1) * R_(0,1) + P_(0,0)(1,0) * R_(1,0) + P_(0,0)(1,1) * R_(1,1) \n",
    "\n",
    "= 0.5 * -3 + 0.25 * -1 + 0.25 * -1 + 0.0 * 1 = -2.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 행동가치함수를 계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동 가치 함수\n",
    "def action_value_function(env, agent, act, G, max_step, now_step):   \n",
    "    \n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # 2. 현재 위치가 목적지인지 확인\n",
    "    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n",
    "        return env.goal\n",
    "\n",
    "    # 3. 마지막 상태는 보상만 계산\n",
    "    if (max_step == now_step):\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += agent.select_action_pr[act]*reward\n",
    "        return G\n",
    "    \n",
    "    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n",
    "    else:\n",
    "        # 4.1현재 위치 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        observation, reward, done = env.move(agent, act)\n",
    "        G += reward\n",
    "        \n",
    "        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n",
    "        if done == True:            \n",
    "            if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n",
    "                agent.set_pos(pos1)\n",
    "            \n",
    "        # 4.3 현재 위치를 다시 저장\n",
    "        pos1 = agent.get_pos()\n",
    "        \n",
    "        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n",
    "        for i in range(len(agent.action)):\n",
    "            agent.set_pos(pos1)\n",
    "            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n",
    "            G += agent.select_action_pr[i] * gamma * next_v\n",
    "        return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_step_number = 0 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -0.75       |     -0.75       |\n",
      "| -0.75     -0.25 | -0.25     -0.75 |\n",
      "|     -0.25       |      0.25       |\n",
      "+-----------------+-----------------+\n",
      "|     -0.25       |      1.00       |\n",
      "| -0.75      0.25 |  1.00      1.00 |\n",
      "|     -0.75       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 1 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -3.45       |     -3.34       |\n",
      "| -3.45     -1.34 | -1.45     -3.34 |\n",
      "|     -1.34       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -1.45       |      1.00       |\n",
      "| -3.34      1.90 |  1.00      1.00 |\n",
      "|     -3.34       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 2 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -5.15       |     -4.40       |\n",
      "| -5.15     -2.40 | -3.15     -4.40 |\n",
      "|     -2.40       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -3.15       |      1.00       |\n",
      "| -4.40      1.90 |  1.00      1.00 |\n",
      "|     -4.40       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 3 total_time = 0.0(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -6.40       |     -5.26       |\n",
      "| -6.40     -3.26 | -4.40     -5.26 |\n",
      "|     -3.26       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -4.40       |      1.00       |\n",
      "| -5.26      1.90 |  1.00      1.00 |\n",
      "|     -5.26       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 4 total_time = 0.01(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -7.35       |     -5.93       |\n",
      "| -7.35     -3.93 | -5.35     -5.93 |\n",
      "|     -3.93       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.35       |      1.00       |\n",
      "| -5.93      1.90 |  1.00      1.00 |\n",
      "|     -5.93       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 5 total_time = 0.05(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -8.08       |     -6.44       |\n",
      "| -8.08     -4.44 | -6.08     -6.44 |\n",
      "|     -4.44       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.08       |      1.00       |\n",
      "| -6.44      1.90 |  1.00      1.00 |\n",
      "|     -6.44       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 6 total_time = 0.15(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -8.63       |     -6.84       |\n",
      "| -8.63     -4.84 | -6.63     -6.84 |\n",
      "|     -4.84       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.63       |      1.00       |\n",
      "| -6.84      1.90 |  1.00      1.00 |\n",
      "|     -6.84       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 7 total_time = 0.51(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.06       |     -7.14       |\n",
      "| -9.06     -5.14 | -7.06     -7.14 |\n",
      "|     -5.14       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.06       |      1.00       |\n",
      "| -7.14      1.90 |  1.00      1.00 |\n",
      "|     -7.14       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 8 total_time = 1.76(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.39       |     -7.38       |\n",
      "| -9.39     -5.38 | -7.39     -7.38 |\n",
      "|     -5.38       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.39       |      1.00       |\n",
      "| -7.38      1.90 |  1.00      1.00 |\n",
      "|     -7.38       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n",
      "max_step_number = 9 total_time = 5.82(s)\n",
      "Q - table\n",
      "+-----------------+-----------------+\n",
      "|     -9.65       |     -7.56       |\n",
      "| -9.65     -5.56 | -7.65     -7.56 |\n",
      "|     -5.56       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.65       |      1.00       |\n",
      "| -7.56      1.90 |  1.00      1.00 |\n",
      "|     -7.56       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. 환경 초기화\n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "# 3. 계산 시간 저장을 위한 list\n",
    "time_len = []\n",
    "\n",
    "max_step_number = 10\n",
    "\n",
    "for max_step in range(max_step_number):\n",
    "    start_time = time.time()\n",
    "    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n",
    "    #print(\"max_step = {}\".format(max_step))\n",
    "    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            # 4.2 모든 행동에 대해\n",
    "            for action in range(len(agent.action)):\n",
    "                # 4.2.1 에이전트의 위치를 초기화\n",
    "                agent.set_pos([i,j])\n",
    "                # 4.2.2 현재 위치에서 행동 가치를 계산\n",
    "                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n",
    "                \n",
    "    time_len.append(time.time()-start_time)\n",
    "    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n",
    "\n",
    "    q = np.round(q_table,2)\n",
    "    print(\"Q - table\")\n",
    "    show_q_table(q, env)\n",
    "    print(\"High actions Arrow\")\n",
    "    show_q_table_arrow(q,env)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정책 평가를 통한 행동 가치 함수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "+-----------------+-----------------+\n",
      "|     -3.00       |     -3.00       |\n",
      "| -3.00     -1.00 | -1.00     -3.00 |\n",
      "|     -1.00       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -1.00       |      1.00       |\n",
      "| -3.00      1.90 |  1.00      1.00 |\n",
      "|     -3.00       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 1\n",
      "+-----------------+-----------------+\n",
      "|     -4.80       |     -4.15       |\n",
      "| -4.80     -2.15 | -2.80     -4.15 |\n",
      "|     -2.15       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -2.80       |      1.00       |\n",
      "| -4.15      1.90 |  1.00      1.00 |\n",
      "|     -4.15       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 2\n",
      "+-----------------+-----------------+\n",
      "|     -6.13       |     -5.07       |\n",
      "| -6.13     -3.07 | -4.13     -5.07 |\n",
      "|     -3.07       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -4.13       |      1.00       |\n",
      "| -5.07      1.90 |  1.00      1.00 |\n",
      "|     -5.07       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 3\n",
      "+-----------------+-----------------+\n",
      "|     -7.14       |     -5.78       |\n",
      "| -7.14     -3.78 | -5.14     -5.78 |\n",
      "|     -3.78       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.14       |      1.00       |\n",
      "| -5.78      1.90 |  1.00      1.00 |\n",
      "|     -5.78       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 4\n",
      "+-----------------+-----------------+\n",
      "|     -7.91       |     -6.33       |\n",
      "| -7.91     -4.33 | -5.91     -6.33 |\n",
      "|     -4.33       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -5.91       |      1.00       |\n",
      "| -6.33      1.90 |  1.00      1.00 |\n",
      "|     -6.33       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 5\n",
      "+-----------------+-----------------+\n",
      "|     -8.51       |     -6.75       |\n",
      "| -8.51     -4.75 | -6.51     -6.75 |\n",
      "|     -4.75       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.51       |      1.00       |\n",
      "| -6.75      1.90 |  1.00      1.00 |\n",
      "|     -6.75       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 6\n",
      "+-----------------+-----------------+\n",
      "|     -8.97       |     -7.08       |\n",
      "| -8.97     -5.08 | -6.97     -7.08 |\n",
      "|     -5.08       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -6.97       |      1.00       |\n",
      "| -7.08      1.90 |  1.00      1.00 |\n",
      "|     -7.08       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 7\n",
      "+-----------------+-----------------+\n",
      "|     -9.32       |     -7.32       |\n",
      "| -9.32     -5.32 | -7.32     -7.32 |\n",
      "|     -5.32       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.32       |      1.00       |\n",
      "| -7.32      1.90 |  1.00      1.00 |\n",
      "|     -7.32       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 8\n",
      "+-----------------+-----------------+\n",
      "|     -9.59       |     -7.52       |\n",
      "| -9.59     -5.52 | -7.59     -7.52 |\n",
      "|     -5.52       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.59       |      1.00       |\n",
      "| -7.52      1.90 |  1.00      1.00 |\n",
      "|     -7.52       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "step 9\n",
      "+-----------------+-----------------+\n",
      "|     -9.80       |     -7.66       |\n",
      "| -9.80     -5.66 | -7.80     -7.66 |\n",
      "|     -5.66       |      1.90       |\n",
      "+-----------------+-----------------+\n",
      "|     -7.80       |      1.00       |\n",
      "| -7.66      1.90 |  1.00      1.00 |\n",
      "|     -7.66       |      1.00       |\n",
      "+-----------------+-----------------+\n",
      "High actions Arrow\n",
      "+-----------------+-----------------+\n",
      "|                 |                 |\n",
      "|          →     |                 |\n",
      "|        ↓       |        ↓       |\n",
      "+-----------------+-----------------+\n",
      "|                 |        ↑       |\n",
      "|          →     |      ←  →     |\n",
      "|                 |        ↓       |\n",
      "+-----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "def update_q_table_once(env, agent, q_table):   \n",
    "    old_q_table = q_table\n",
    "    old_v_table = np.zeros([env.reward.shape[0], env.reward.shape[1]])\n",
    "    new_q_table = np.copy(q_table)\n",
    "    # 1. 감가율 설정\n",
    "    gamma = 0.9\n",
    "    \n",
    "    \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            if env.reward_list1[i][j] == \"goal\":\n",
    "                old_v_table[i][j] = env.goal\n",
    "            else:\n",
    "                for action in range(len(agent.action)):\n",
    "                    old_v_table[i][j] +=  agent.select_action_pr[action] * old_q_table[i][j][action]\n",
    "    \n",
    "    \n",
    "    for i in range(env.reward.shape[0]):\n",
    "        for j in range(env.reward.shape[1]):\n",
    "            for action in range(len(agent.action)):\n",
    "                if env.reward_list1[i][j] == \"goal\":\n",
    "                    new_q_table[i][j][action] = env.goal\n",
    "                    continue\n",
    "                agent.set_pos([i, j])\n",
    "                observation, reward, done = env.move(agent, action)\n",
    "                next_pos = agent.get_pos()\n",
    "                new_q_table[i][j][action] = reward + gamma * old_v_table[next_pos[0], next_pos[1]]\n",
    "    return new_q_table\n",
    "\n",
    "  \n",
    "env = Environment()\n",
    "\n",
    "# 2. 에이전트 초기화\n",
    "agent = Agent()\n",
    "np.random.seed(0)\n",
    "\n",
    "q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n",
    "\n",
    "for i in range(10):\n",
    "  print(\"step {}\".format(i))\n",
    "  q_table = update_q_table_once(env, agent, q_table)\n",
    "  \n",
    "  show_q_table(np.round(q_table, 2), env)\n",
    "  print(\"High actions Arrow\")\n",
    "  show_q_table_arrow(np.round(q_table, 2),env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 코드까지는 각 (상태, 행동) 쌍에서 행동 가치 함수를 구하는 두 가지 과정이다.\n",
    "\n",
    "\n",
    "### 질문 2-1. 재귀적으로 구하는 과정과 정책 평가로 구하는 두 과정을 비교해보자. 구하는 과정은 어떤 점에서 차이가 있는가? 두 행동 가치 함수 테이블은 가까워 지는가? 무엇이 더 좋은 방법이라고 생각하는가? 그 이유는 무엇인가?\n",
    "\n",
    "### 질문 2-2. 왼쪽 상단, 즉 (0, 0)에서 에이전트가 출발한다고 생각해보자. 어떻게 행동을 취하면 Goal에 도착하는가? 가능한 행동의 리스트를 한 가지 경우만 제시해보자. 어떠한 기준으로 행동을 선택하였는가?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
